[
  {
    "test_name": "latency_llama8B_tp1",
    "parameters": {
      "model": "LLAMA_3_1_8B_PATH",
      "tensor_parallel_size": 1,
      "load_format": "dummy",
      "num_iters_warmup": 5,
      "num_iters": 15
    }
  }
]
